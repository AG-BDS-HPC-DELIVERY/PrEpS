= PrEpS (**Pr**olog/**Ep**ilog for **S**lurm)

== About the Project

PrEpS (**Pr**olog/**Ep**ilog for **S**lurm) is a comprehensive but extensible framework for performing Prolog/Epilog execution inside the Slurm workload scheduler.

Reference documentation for the Slurm Prolog/Epilog mechanism is available here:
https://slurm.schedmd.com/prolog_epilog.html[SchedMD: Prolog / Epilog]

The various Prolog/Epilog mechanisms are summarized in the table below:

|===
|Stage|Slurm Context|Configuration Parameter|Location|User

|Slurm Controller Prolog
|`prolog_slurmctld`
|`PrologSlurmctld`
|Head Node
|SlurmctldUser (`slurm`)

|Slurm Controller Epilog
|`epilog_slurmctld`
|`EpilogSlurmctld`
|Head Node
|SlurmctldUser (`slurm`)

|Slurm Prolog
|`prolog_slurmd`
|`Prolog`
|Compute Node
|SlurmdUser (`root`)

|Slurm Epilog
|`epilog_slurmd`
|`Epilog`
|Compute Node
|SlurmdUser (`root`)

|srun Prolog
|`prolog_srun`
|`SrunProlog` +
(`srun --prolog`)
|srun Invocation Node
|srun Invocation User

|srun Epilog
|`epilog_srun`
|`SrunEpilog` +
(`srun --epilog`)
|srun Invocation Node
|srun Invocation User

|Task Prolog
|`prolog_task`
|`TaskProlog` +
(`srun --task-prolog`)
|Compute Node
|srun Invocation User

|Task Epilog
|`epilog_task`
|`TaskEpilog` +
(`srun --task-epilog`)
|Compute Node
|srun Invocation User
|===

== Software Architecture

=== Scripts

PrEpS main script is located in:
----
<PREFIX>/preps/bin/preps
----

This script is a single entry point which takes care of calling the proper prolog/epilog script depending on the actual Slurm context:

|===
|Stage|Slurm Context|Script

|Slurm Controller Prolog
|`prolog_slurmctld`
|`<PREFIX>/preps/bin/preps.prolog_slurmctld`

|Slurm Controller Epilog
|`epilog_slurmctld`
|`<PREFIX>/preps/bin/preps.epilog_slurmctld`

|Slurm Prolog
|`prolog_slurmd`
|`<PREFIX>/preps/bin/preps.prolog_slurmd`

|Slurm Epilog
|`epilog_slurmd`
|`<PREFIX>/preps/bin/preps.epilog_slurmd`

|srun Prolog
|`prolog_srun`
|`<PREFIX>/preps/bin/preps.prolog_srun`

|srun Epilog
|`epilog_srun`
|`<PREFIX>/preps/bin/preps.epilog_srun`

|Task Prolog
|`prolog_task`
|`<PREFIX>/preps/bin/preps.prolog_task`

|Task Epilog
|`epilog_task`
|`<PREFIX>/preps/bin/preps.epilog_task`
|===

=== Library

==== Packages

PrEpS is divided into several distinct, complementary packages:

|===
|Package|Purpose

|CPU Frequency
|Manage CPU frequency

|Filesystems
|Manage filesystems

|Memory
|Manage system memory

|NVIDIA
|Manage NVIDIA GPU configuration

|Operating System
|Manage Operating System components

|systemd
|Manage systemd units

|Transparent Huge Pages
|Manage Transparent Huge Pages (THP) configuration

|tuned
|Manage tuned profiles
|===

==== Modules

Each PrEpS package is divided into several distinct modules.

===== CPU Frequency

The package offers the following modules:

[cols="1,1,2",options="header",width="100%"]
|===
|Function Name|Arguments|Purpose

|`preps::cpufreq::set_boost`
|`{0 \| 1}`
|Set CPU boost (Intel Turbo Boost / AMD Core Performance Boost)

|`preps::cpufreq::set_frequency_min_max`
|`<FREQ_MIN>:<FREQ_MAX>`
|Set CPU frequency Min./Max.

|`preps::cpufreq::set_frequency`
|`<FREQ>`
|Set CPU frequency

|`preps::cpufreq::set_governor`
|`<GOVERNOR>`
|Set CPU governor
|===

===== Filesystems

The package offers the following modules:

[cols="1,1,2",options="header",width="100%"]
|===
|Function Name|Arguments|Purpose

|`preps::fs::check_mountpoints`
|`MOUNTPOINT[,MOUNTPOINT]`
|Check mountpoints

|`preps::fs::cleanup_tmpdirs`
|`TMPDIR[,TMPDIR]`
|Cleanup temporary directory
|===

===== Memory

The package offers the following modules:

[cols="1,1,2",options="header",width="100%"]
|===
|Function Name|Arguments|Purpose

|`preps::memory::cleanup_ipc`
|-
|Cleanup existing Inter-Process Communications (IPC)

|`preps::memory::cleanup_memory_caches`
|`{1: pagecache | 2: dentries + inodes | 3: pagecache + dentries + inodes}`
|Cleanup memory caches

|`preps::memory::cleanup_shared_memory_segments`
|-
|Cleanup residual shared memory segments

|`preps::memory::compact_memory`
|-
|Compact memory

|`preps::memory::set_numa_balancing`
|`{0 \| 1}`
|Set NUMA balancing
|===

===== NVIDIA

The package offers the following modules:

[cols="1,1,2",options="header",width="100%"]
|===
|Function Name|Arguments|Purpose

|`preps::nvidia::check_pending_retirements`
|-
|Check pending retirements

|`preps::nvidia::check_residual_user_processes`
|-
|Check residual user processes

|`preps::nvidia::cleanup_mps_processes`
|-
|Cleanup residual MPS processes

|`preps::nvidia::reset_applications_clocks`
|-
|Reset applications clocks

|`preps::nvidia::run_dcgm_diag`
|`{1 \| 2 \| 3}`
|Run DCGM diagnostic with specified level

|`preps::nvidia::set_applications_clocks`
|`MEMORY,GRAPHICS`
|Set applications clocks

|`preps::nvidia::set_persistence_mode`
|`{0 \| 1}`
|Set persistence mode

|`preps::nvidia::set_power_limit`
|`<POWER_LIMIT>`
|Set power limit

|`preps::nvidia::set_vboost_slider`
|`{0 \| 1 \| 2 \| 3 \| 4}`
|Set vboost Slider
|===

===== Operating System

The package offers the following modules:

[cols="1,1,2",options="header",width="100%"]
|===
|Function Name|Arguments|Purpose

|`preps::os::check_residual_user_processes`
|-
|Check residual user processes

|`preps::os::cleanup_residual_user_processes`
|-
|Cleanup residual user processes

|`preps::os::set_user_limits`
|`OPTION=VALUE[,OPTION=VALUE]`
|Set user limits
|===

===== systemd

The package offers the following modules:

[cols="1,1,2",options="header",width="100%"]
|===
|Function Name|Arguments|Purpose

|`preps::systemd::reload_systemd_unit`
|`UNIT` +
`MAX_ACTIVE_TIME_IN_SECONDS`
|Reload systemd unit if active time is above specified threshold

|`preps::systemd::start_failed_systemd_units`
|-
|Start failed systemd units

|`preps::systemd::start_systemd_units`
|`UNIT[,UNIT]`
|Start systemd units

|`preps::systemd::stop_systemd_units`
|`UNIT[,UNIT]`
|Stop systemd units
|===

===== Transparent Huge Pages (THP)

The package offers the following modules:

[cols="1,1,2",options="header",width="100%"]
|===
|Function Name|Arguments|Purpose

|`preps::thp::set_thp`
|`{always \| madvise \| never}`
|Set Transparent Huge Pages (THP) mode

|`preps::thp::set_thp_shmem`
|`{always \| deny \| force \| madvise \| never}`
|Set Transparent Huge Pages for SHMEM (THP SHMEM) mode
|===

===== tuned

The package offers the following modules:

[cols="1,1,2",options="header",width="100%"]
|===
|Function Name|Arguments|Purpose

|`preps::tuned::enable_tuned_profile`
|`PROFILE`
|Apply tuned profile
|===

== Getting Started

=== Prerequisites

* Functional Slurm workload manager

=== Installation

* Clone Git repository:
+
----
git clone https://github.com/AG-BDS-HPC-DELIVERY/PrEpS/preps.git
----
* Grant proper access rights to the PrEpS directory tree:
+
----
chmod --recursive g=u,g-w,o=g <PREFIX>/preps
----
* Grant executive access rights to the PrEpS binary:
+
----
chmod a+x <PREFIX>/preps/bin/preps
----

=== Configuration

* Define PrEpS executable binary as the target for Prolog/Epilog inside Slurm configuration file `slurm.conf`:
+
----
################################################
#               EPILOG & PROLOG                #
################################################
Epilog=<PREFIX>/preps/bin/preps
Prolog=<PREFIX>/preps/bin/preps
----
+
[NOTE]
====
The PrEpS installation directory must be located on a shared filesystem in order to be accessible from every compute node.
====
* Launch Slurm reconfiguration:
+
----
scontrol reconfig
----
* Check that the Prolog/Epilog are properly configured:
+
----
scontrol show config  | awk '/^Epilog\s+=/ || /^Prolog\s+=/'
----
* Build associated Prolog/Epilog scripts by populating files with requested function calls:
+
Example: `preps.prolog_slurmd` Script for Slurm Prolog:
+
----
#!/usr/bin/env bash

preps::fs::check_mountpoints "/home"
preps::nvidia::check_residual_user_processes
preps::nvidia::set_power_limit "${NVIDIA_POWER_LIMIT:-680}"
preps::nvidia::set_vboost_slider "${NVIDIA_VBOOST_SLIDER:-0}"
preps::thp::set_thp "${THP:-always}"

----
* Setup optional override of predefined settings whenever required using the following notation as arguments to the function calls:
+
----
preps::nvidia::set_power_limit "${NVIDIA_POWER_LIMIT:-680}"
----
+
Explanation: Default power limit value is 680, but this value can be overriden by the user on a per-job basis.

== Usage

=== Logging

The log level is defined according to the following elements **in decreasing level of priority**:

. Value from the `LOG_LEVEL` environment variable (See Section: Overrides) -> Scope: Job-specific.
. Value from the `-log-level` argument to the PrEpS executable -> Scope: Prolog/Epilog-specific
. Default value inside the PrEpS executable -> Scope: Global.
+
NOTE: Default log level is set to `ERROR` in order to minimize the size of the logfile.

The execution of Prolog/Epilog is logged into the following files:
----
<PREFIX>/preps/var/log/preps.<DD>.log
----

Log file records are made of the following fields:

[cols="1,2",options="header",width="100%"]
|===
|Field Name|Purpose

|Timestamp
|Date and time of the event

|Slurm Job ID
|Slurm ID of the job

|Slurm User Name
|Name of the submission user

|Hostname
|Name of the compute node

|Slurm Context
|Prolog/Epilog context

|Log Level
|Log level of the associated message: Trace, Debugging, Information, Warning, Error, Critical

|Message
|Content of the message

|===


Log File Example::

----
2024-07-25 12:00:47 +02:00 |     3238 | patkinson       | pm10-nod48      | prolog_slurmd        | DEBUG | Performing Execution under User Account: [root] on Behalf of User Account: [patkinson]
2024-07-25 12:00:48 +02:00 |     3238 | patkinson       | pm10-nod68      | prolog_slurmd        | DEBUG | Performing Execution under User Account: [root] on Behalf of User Account: [patkinson]
2024-07-25 12:00:55 +02:00 |     3238 | patkinson       | pm10-nod48      | prolog_slurmd        | INFO  | Mountpoint: [/home] Mounted
2024-07-25 12:00:57 +02:00 |     3238 | patkinson       | pm10-nod68      | prolog_slurmd        | INFO  | Mountpoint: [/home] Mounted
2024-07-25 12:00:59 +02:00 |     3238 | patkinson       | pm10-nod48      | prolog_slurmd        | INFO  | No Residual User Process Found
2024-07-25 12:00:59 +02:00 |     3238 | patkinson       | pm10-nod68      | prolog_slurmd        | INFO  | No Residual User Process Found
2024-07-25 12:01:00 +02:00 |     3238 | patkinson       | pm10-nod48      | prolog_slurmd        | INFO  | Set Power Limit: [680.00 | 680.00 | 680.00 | 680.00]
2024-07-25 12:01:02 +02:00 |     3238 | patkinson       | pm10-nod68      | prolog_slurmd        | INFO  | Set Power Limit: [680.00 | 680.00 | 680.00 | 680.00]
2024-07-25 12:01:09 +02:00 |     3238 | patkinson       | pm10-nod48      | prolog_slurmd        | INFO  | Set Video Boost Slider: [0 | 0 | 0 | 0]
2024-07-25 12:01:10 +02:00 |     3238 | patkinson       | pm10-nod68      | prolog_slurmd        | INFO  | Set Video Boost Slider: [0 | 0 | 0 | 0]
2024-07-25 12:01:12 +02:00 |     3238 | patkinson       | pm10-nod48      | prolog_slurmd        | INFO  | Set Transparent Huge Pages (THP): [[always] madvise never]
2024-07-25 12:01:15 +02:00 |     3238 | patkinson       | pm10-nod68      | prolog_slurmd        | INFO  | Set Transparent Huge Pages (THP): [[always] madvise never]
----

=== Overrides

Overrides are designed to allow regular users to alter the configuration of the compute nodes at submission time, with no need to be granted sudo rights.

Overrides rely on environment variables holding user-specific values.

==== Passing Variables to the Prolog/Epilog

The Prolog/Epilog environment **does not** inherit from the current user environment at submission time.

Therefore, in order to pass environment variables to the Prolog/Epilog, it is required to use the `--comment` directive of the `sbatch` command in either of the following two ways:

* As a direct argument to the `sbatch` command itself:
+
----
sbatch --comment "NVIDIA_POWER_LIMIT=680" <...>
----
* As an explicit directive inside the job submission file:
+
----
#SBATCH --comment="NVIDIA_POWER_LIMIT=680"
----

==== Supported Overrides

[cols="1,1,2",options="header",width="100%"]
|===
|Category|Environment Variable Name|Purpose

|PrEpS
|`LOG_LEVEL`
|Job-specific Log Level: {TRACE \| DEBUG \| INFO \| WARN \| ERROR \| FATAL}

|NVIDIA
|`NVIDIA_POWER_LIMIT`
|Power Limit of the CG Module/GPU Devices in Watts

|
|`NVIDIA_VBOOST_SLIDER`
|Configuration of the Video Boost Slider: {0 \| 1 \| 2 \| 3 \| 4}

|THP
|`THP`
|Configuration of Transparent Huge Pages (THP): {always \| madvise \| never}

|
|`THP_SHMEM`
|Configuration of Transparent Huge Pages (THP) for SHMEM: {advise \| always \| deny \| force \| never}

|===
